{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6525c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c9e0df",
   "metadata": {},
   "source": [
    "# Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple: (filters, size, stride), single convolutional layer\n",
    "\n",
    "# [\"residual\", repetitions]: residual layers, let prev_channel be the number of channels of the previous layer,\n",
    "# the sequence of conv with filters of size: prev_channels//2 ---> prev_channels ---> residual connection\n",
    "# is repeated for \"repetitions\" times\n",
    "\n",
    "# [\"residualYolo\", repetitions]: same as \"residual\" but some feature maps are saved to be used with the YOLO network\n",
    "\n",
    "# [\"avgpool\"]: avearge pooling with output of size 1x1 for each channel\n",
    "\n",
    "# [\"prediction\"]: last convolutional layer which produces a number of channels equal to the number of classes\n",
    "\n",
    "# [\"softmax\"]: apply softmax to predictions\n",
    "\n",
    "darknet_architecture = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"residual\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"residual\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"residualYolo\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"residualYolo\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"residual\", 4],\n",
    "    [\"avgpool\"],\n",
    "    [\"prediction\"],\n",
    "    [\"softmax\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74073eab",
   "metadata": {},
   "source": [
    "# Blocks Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        padding=1 if kernel_size == 3 else 0\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              bias=not bn_act,\n",
    "                              padding=padding, \n",
    "                              **kwargs)\n",
    "        # if batchnorm, then leaky relu as activation function\n",
    "        self.use_bn_act = bn_act\n",
    "        if self.use_bn_act:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "            self.leaky = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    ConvLayer(channels, channels // 2, kernel_size=1),\n",
    "                    ConvLayer(channels // 2, channels, kernel_size=3),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + self.use_residual * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13153b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block of convolutional layers with feature map to be saved for detections\n",
    "class ConvBlockYolo(nn.Module):\n",
    "    def __init__(self, channels, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.feat_map = None\n",
    "        num_layer = 1\n",
    "        for i in range(num_repeats):\n",
    "            if num_layer == 5:\n",
    "                self.layers += [\n",
    "                    ConvLayer(channels, channels // 2, kernel_size=1),\n",
    "                    ConvLayer(channels // 2, channels, kernel_size=3)\n",
    "                ]\n",
    "            else:\n",
    "                self.layers += [\n",
    "                    nn.Sequential(\n",
    "                        ConvLayer(channels, channels // 2, kernel_size=1),\n",
    "                        ConvLayer(channels // 2, channels, kernel_size=3),\n",
    "                    )\n",
    "                ]\n",
    "            num_layer += 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.feat_map = None\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ConvLayer):\n",
    "                if self.feat_map == None:\n",
    "                    self.feat_map = layer(x)\n",
    "                else:\n",
    "                    x = layer(self.feat_map) + x\n",
    "            else:\n",
    "                x = layer(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cc016",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca12e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class darknet53(nn.Module):\n",
    "    def __init__(self, architecture, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_layers(architecture)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.reshape(x.shape[0], x.shape[1])\n",
    "        return x#self.softmax(x)\n",
    "        \n",
    "    def _create_layers(self, architecture):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "        for module in architecture:\n",
    "            # conv layer\n",
    "            if isinstance(module, tuple):\n",
    "                if module[-1] == \"linear\":\n",
    "                    bn_act = False\n",
    "                else:\n",
    "                    bn_act = True\n",
    "                layers.append(ConvLayer(in_channels, \n",
    "                                        module[0], \n",
    "                                        kernel_size=module[1],\n",
    "                                        bn_act=bn_act,\n",
    "                                        stride=module[2])\n",
    "                             )\n",
    "                in_channels = module[0]\n",
    "                continue\n",
    "            \n",
    "            if isinstance(module, list):\n",
    "                # residual block\n",
    "                if module[0] == \"residual\":\n",
    "                    layers.append(ConvBlock(in_channels,\n",
    "                                            num_repeats=module[1])\n",
    "                                 )\n",
    "                    continue\n",
    "                    \n",
    "                # residual block with feature map to be saved for detection\n",
    "                elif module[0] == \"residualYolo\":\n",
    "                    layers.append(ConvBlockYolo(in_channels,\n",
    "                                            num_repeats=module[1])\n",
    "                                 )\n",
    "                \n",
    "                # average pool\n",
    "                elif module[0] == \"avgpool\":\n",
    "                    layers.append(nn.AdaptiveAvgPool2d((1,1)))\n",
    "                    continue\n",
    "                \n",
    "                # last convolutional layer\n",
    "                elif module[0] == \"prediction\":\n",
    "                    layers.append(ConvLayer(in_channels,\n",
    "                                            self.num_classes,\n",
    "                                            kernel_size=1,\n",
    "                                            bn_act=False,\n",
    "                                            stride=1)\n",
    "                                 )\n",
    "                    continue\n",
    "                    \n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a01b97b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1627a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgnet_trainset(Dataset):\n",
    "    def __init__(self, df, path=\"./ImageNet_darknet_dataset/data/images\"):\n",
    "        self.transform = transforms.PILToTensor()\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.path, row[\"filename\"].split(\"_\")[0], row[\"filename\"]))\n",
    "        return (self.transform(img).float()/255, row[\"label\"]) #torch.tensor([row[\"label\"]]))\n",
    "    \n",
    "class imgnet_testset(Dataset):\n",
    "    def __init__(self, df, path=\"./ImageNet_darknet_dataset/data/images\"):\n",
    "        self.transform = transforms.PILToTensor()\n",
    "        self.df = df\n",
    "        self.path = path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(os.path.join(self.path, row[\"filename\"].split(\"_\")[0], row[\"filename\"]))\n",
    "        return (self.transform(img).float()/255, row[\"label\"])#torch.tensor([row[\"label\"]]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e35ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path=\"./ImageNet_darknet_dataset\"):\n",
    "    # in trainset, occurrences of classes goes from a max of 1200 to a minimum of 632, mean: 499\n",
    "    df_main = pd.read_csv(os.path.join(path, \"main.csv\"), sep=\";\", skipinitialspace=True)\n",
    "    testset_df = pd.DataFrame(columns=[\"filename\", \"label\"])\n",
    "    trainset_df = pd.DataFrame(columns=[\"filename\", \"label\"])\n",
    "    for idx in range(len(df_main)):\n",
    "        print(f\"CLASS {idx+1}/1000\")\n",
    "        row = df_main.iloc[idx]\n",
    "        path_class_csv = os.path.join(path, \"data\", \"info_classes\", f\"class_{row['label']}.csv\")\n",
    "        df_cls = pd.read_csv(path_class_csv, sep=\";\", skipinitialspace=True)\n",
    "        # for trainset sample 100 images for each class, trainset size is 100k\n",
    "        df_testset_sample = df_cls.sample(100)\n",
    "        df_cls = df_cls.drop(list(df_testset_sample.index.values))\n",
    "        df_cls = df_cls.sample(500)\n",
    "        # add splitted dataframes to trainset and testset dataframes\n",
    "        testset_df = pd.concat([testset_df, df_testset_sample], axis=0)\n",
    "        trainset_df = pd.concat([trainset_df, df_cls], axis=0)\n",
    "    return imgnet_trainset(trainset_df), imgnet_testset(testset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335c4aa",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "augmentation = A.Compose(\n",
    "    [\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform agumentation when loading batch\n",
    "def collate_fn_padd(batch):\n",
    "    X = None\n",
    "    Y = None\n",
    "    for img, y in batch:\n",
    "        img_numpy = (torch.permute(img, (1, 2, 0))*255).numpy().astype(np.uint8)\n",
    "        img_numpy = augmentation(image=img_numpy)[\"image\"]\n",
    "        tensor_augm = torch.permute(torch.from_numpy(img_numpy), (2, 0, 1)) / 255\n",
    "        if X == None:\n",
    "            X = torch.stack((img, tensor_augm))\n",
    "            Y = torch.cat((y, y))\n",
    "        else:\n",
    "            X = torch.cat((X, img.unsqueeze(0), tensor_augm.unsqueeze(0)))\n",
    "            Y = torch.cat((Y, y, y))\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062310a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 1000\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c090d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS 559/1000\n",
      "CLASS 560/1000\n",
      "CLASS 561/1000\n",
      "CLASS 562/1000\n",
      "CLASS 563/1000\n",
      "CLASS 564/1000\n",
      "CLASS 565/1000\n",
      "CLASS 566/1000\n",
      "CLASS 567/1000\n",
      "CLASS 568/1000\n",
      "CLASS 569/1000\n",
      "CLASS 570/1000\n",
      "CLASS 571/1000\n",
      "CLASS 572/1000\n",
      "CLASS 573/1000\n",
      "CLASS 574/1000\n",
      "CLASS 575/1000\n",
      "CLASS 576/1000\n",
      "CLASS 577/1000\n",
      "CLASS 578/1000\n",
      "CLASS 579/1000\n",
      "CLASS 580/1000\n",
      "CLASS 581/1000\n",
      "CLASS 582/1000\n",
      "CLASS 583/1000\n",
      "CLASS 584/1000\n",
      "CLASS 585/1000\n",
      "CLASS 586/1000\n",
      "CLASS 587/1000\n",
      "CLASS 588/1000\n",
      "CLASS 589/1000\n",
      "CLASS 590/1000\n",
      "CLASS 591/1000\n",
      "CLASS 592/1000\n",
      "CLASS 593/1000\n",
      "CLASS 594/1000\n",
      "CLASS 595/1000\n",
      "CLASS 596/1000\n",
      "CLASS 597/1000\n",
      "CLASS 598/1000\n",
      "CLASS 599/1000\n",
      "CLASS 600/1000\n",
      "CLASS 601/1000\n",
      "CLASS 602/1000\n",
      "CLASS 603/1000\n",
      "CLASS 604/1000\n",
      "CLASS 605/1000\n",
      "CLASS 606/1000\n",
      "CLASS 607/1000\n",
      "CLASS 608/1000\n",
      "CLASS 609/1000\n",
      "CLASS 610/1000\n",
      "CLASS 611/1000\n",
      "CLASS 612/1000\n",
      "CLASS 613/1000\n",
      "CLASS 614/1000\n",
      "CLASS 615/1000\n",
      "CLASS 616/1000\n",
      "CLASS 617/1000\n",
      "CLASS 618/1000\n",
      "CLASS 619/1000\n",
      "CLASS 620/1000\n",
      "CLASS 621/1000\n",
      "CLASS 622/1000\n",
      "CLASS 623/1000\n",
      "CLASS 624/1000\n",
      "CLASS 625/1000\n",
      "CLASS 626/1000\n",
      "CLASS 627/1000\n",
      "CLASS 628/1000\n",
      "CLASS 629/1000\n",
      "CLASS 630/1000\n",
      "CLASS 631/1000\n",
      "CLASS 632/1000\n",
      "CLASS 633/1000\n",
      "CLASS 634/1000\n",
      "CLASS 635/1000\n",
      "CLASS 636/1000\n",
      "CLASS 637/1000\n",
      "CLASS 638/1000\n",
      "CLASS 639/1000\n",
      "CLASS 640/1000\n",
      "CLASS 641/1000\n",
      "CLASS 642/1000\n",
      "CLASS 643/1000\n",
      "CLASS 644/1000\n",
      "CLASS 645/1000\n",
      "CLASS 646/1000\n",
      "CLASS 647/1000\n",
      "CLASS 648/1000\n",
      "CLASS 649/1000\n",
      "CLASS 650/1000\n",
      "CLASS 651/1000\n",
      "CLASS 652/1000\n",
      "CLASS 653/1000\n",
      "CLASS 654/1000\n",
      "CLASS 655/1000\n",
      "CLASS 656/1000\n",
      "CLASS 657/1000\n",
      "CLASS 658/1000\n",
      "CLASS 659/1000\n",
      "CLASS 660/1000\n",
      "CLASS 661/1000\n",
      "CLASS 662/1000\n",
      "CLASS 663/1000\n",
      "CLASS 664/1000\n",
      "CLASS 665/1000\n",
      "CLASS 666/1000\n",
      "CLASS 667/1000\n",
      "CLASS 668/1000\n",
      "CLASS 669/1000\n",
      "CLASS 670/1000\n",
      "CLASS 671/1000\n",
      "CLASS 672/1000\n",
      "CLASS 673/1000\n",
      "CLASS 674/1000\n",
      "CLASS 675/1000\n",
      "CLASS 676/1000\n",
      "CLASS 677/1000\n",
      "CLASS 678/1000\n",
      "CLASS 679/1000\n",
      "CLASS 680/1000\n",
      "CLASS 681/1000\n",
      "CLASS 682/1000\n",
      "CLASS 683/1000\n",
      "CLASS 684/1000\n",
      "CLASS 685/1000\n",
      "CLASS 686/1000\n",
      "CLASS 687/1000\n",
      "CLASS 688/1000\n",
      "CLASS 689/1000\n",
      "CLASS 690/1000\n",
      "CLASS 691/1000\n",
      "CLASS 692/1000\n",
      "CLASS 693/1000\n",
      "CLASS 694/1000\n",
      "CLASS 695/1000\n",
      "CLASS 696/1000\n",
      "CLASS 697/1000\n",
      "CLASS 698/1000\n",
      "CLASS 699/1000\n",
      "CLASS 700/1000\n",
      "CLASS 701/1000\n",
      "CLASS 702/1000\n",
      "CLASS 703/1000\n",
      "CLASS 704/1000\n",
      "CLASS 705/1000\n",
      "CLASS 706/1000\n",
      "CLASS 707/1000\n",
      "CLASS 708/1000\n",
      "CLASS 709/1000\n",
      "CLASS 710/1000\n",
      "CLASS 711/1000\n",
      "CLASS 712/1000\n",
      "CLASS 713/1000\n",
      "CLASS 714/1000\n",
      "CLASS 715/1000\n",
      "CLASS 716/1000\n",
      "CLASS 717/1000\n",
      "CLASS 718/1000\n",
      "CLASS 719/1000\n",
      "CLASS 720/1000\n",
      "CLASS 721/1000\n",
      "CLASS 722/1000\n",
      "CLASS 723/1000\n",
      "CLASS 724/1000\n",
      "CLASS 725/1000\n",
      "CLASS 726/1000\n",
      "CLASS 727/1000\n",
      "CLASS 728/1000\n",
      "CLASS 729/1000\n",
      "CLASS 730/1000\n",
      "CLASS 731/1000\n",
      "CLASS 732/1000\n",
      "CLASS 733/1000\n",
      "CLASS 734/1000\n",
      "CLASS 735/1000\n",
      "CLASS 736/1000\n",
      "CLASS 737/1000\n",
      "CLASS 738/1000\n",
      "CLASS 739/1000\n",
      "CLASS 740/1000\n",
      "CLASS 741/1000\n",
      "CLASS 742/1000\n",
      "CLASS 743/1000\n",
      "CLASS 744/1000\n",
      "CLASS 745/1000\n",
      "CLASS 746/1000\n",
      "CLASS 747/1000\n",
      "CLASS 748/1000\n",
      "CLASS 749/1000\n",
      "CLASS 750/1000\n",
      "CLASS 751/1000\n",
      "CLASS 752/1000\n",
      "CLASS 753/1000\n",
      "CLASS 754/1000\n",
      "CLASS 755/1000\n",
      "CLASS 756/1000\n",
      "CLASS 757/1000\n",
      "CLASS 758/1000\n",
      "CLASS 759/1000\n",
      "CLASS 760/1000\n",
      "CLASS 761/1000\n",
      "CLASS 762/1000\n",
      "CLASS 763/1000\n",
      "CLASS 764/1000\n",
      "CLASS 765/1000\n",
      "CLASS 766/1000\n",
      "CLASS 767/1000\n",
      "CLASS 768/1000\n",
      "CLASS 769/1000\n",
      "CLASS 770/1000\n",
      "CLASS 771/1000\n",
      "CLASS 772/1000\n",
      "CLASS 773/1000\n",
      "CLASS 774/1000\n",
      "CLASS 775/1000\n",
      "CLASS 776/1000\n",
      "CLASS 777/1000\n",
      "CLASS 778/1000\n",
      "CLASS 779/1000\n",
      "CLASS 780/1000\n",
      "CLASS 781/1000\n",
      "CLASS 782/1000\n",
      "CLASS 783/1000\n",
      "CLASS 784/1000\n",
      "CLASS 785/1000\n",
      "CLASS 786/1000\n",
      "CLASS 787/1000\n",
      "CLASS 788/1000\n",
      "CLASS 789/1000\n",
      "CLASS 790/1000\n",
      "CLASS 791/1000\n",
      "CLASS 792/1000\n",
      "CLASS 793/1000\n",
      "CLASS 794/1000\n",
      "CLASS 795/1000\n",
      "CLASS 796/1000\n",
      "CLASS 797/1000\n",
      "CLASS 798/1000\n",
      "CLASS 799/1000\n",
      "CLASS 800/1000\n",
      "CLASS 801/1000\n",
      "CLASS 802/1000\n",
      "CLASS 803/1000\n",
      "CLASS 804/1000\n",
      "CLASS 805/1000\n",
      "CLASS 806/1000\n",
      "CLASS 807/1000\n",
      "CLASS 808/1000\n",
      "CLASS 809/1000\n",
      "CLASS 810/1000\n",
      "CLASS 811/1000\n",
      "CLASS 812/1000\n",
      "CLASS 813/1000\n",
      "CLASS 814/1000\n",
      "CLASS 815/1000\n",
      "CLASS 816/1000\n",
      "CLASS 817/1000\n",
      "CLASS 818/1000\n",
      "CLASS 819/1000\n",
      "CLASS 820/1000\n",
      "CLASS 821/1000\n",
      "CLASS 822/1000\n",
      "CLASS 823/1000\n",
      "CLASS 824/1000\n",
      "CLASS 825/1000\n",
      "CLASS 826/1000\n",
      "CLASS 827/1000\n",
      "CLASS 828/1000\n",
      "CLASS 829/1000\n",
      "CLASS 830/1000\n",
      "CLASS 831/1000\n",
      "CLASS 832/1000\n",
      "CLASS 833/1000\n",
      "CLASS 834/1000\n",
      "CLASS 835/1000\n",
      "CLASS 836/1000\n",
      "CLASS 837/1000\n",
      "CLASS 838/1000\n",
      "CLASS 839/1000\n",
      "CLASS 840/1000\n",
      "CLASS 841/1000\n",
      "CLASS 842/1000\n",
      "CLASS 843/1000\n",
      "CLASS 844/1000\n",
      "CLASS 845/1000\n",
      "CLASS 846/1000\n",
      "CLASS 847/1000\n",
      "CLASS 848/1000\n",
      "CLASS 849/1000\n",
      "CLASS 850/1000\n",
      "CLASS 851/1000\n",
      "CLASS 852/1000\n",
      "CLASS 853/1000\n",
      "CLASS 854/1000\n",
      "CLASS 855/1000\n",
      "CLASS 856/1000\n",
      "CLASS 857/1000\n",
      "CLASS 858/1000\n",
      "CLASS 859/1000\n",
      "CLASS 860/1000\n",
      "CLASS 861/1000\n",
      "CLASS 862/1000\n",
      "CLASS 863/1000\n",
      "CLASS 864/1000\n",
      "CLASS 865/1000\n",
      "CLASS 866/1000\n",
      "CLASS 867/1000\n",
      "CLASS 868/1000\n",
      "CLASS 869/1000\n",
      "CLASS 870/1000\n",
      "CLASS 871/1000\n",
      "CLASS 872/1000\n",
      "CLASS 873/1000\n",
      "CLASS 874/1000\n",
      "CLASS 875/1000\n",
      "CLASS 876/1000\n",
      "CLASS 877/1000\n",
      "CLASS 878/1000\n",
      "CLASS 879/1000\n",
      "CLASS 880/1000\n",
      "CLASS 881/1000\n",
      "CLASS 882/1000\n",
      "CLASS 883/1000\n",
      "CLASS 884/1000\n",
      "CLASS 885/1000\n",
      "CLASS 886/1000\n",
      "CLASS 887/1000\n",
      "CLASS 888/1000\n",
      "CLASS 889/1000\n",
      "CLASS 890/1000\n",
      "CLASS 891/1000\n",
      "CLASS 892/1000\n",
      "CLASS 893/1000\n",
      "CLASS 894/1000\n",
      "CLASS 895/1000\n",
      "CLASS 896/1000\n",
      "CLASS 897/1000\n",
      "CLASS 898/1000\n",
      "CLASS 899/1000\n",
      "CLASS 900/1000\n",
      "CLASS 901/1000\n",
      "CLASS 902/1000\n",
      "CLASS 903/1000\n",
      "CLASS 904/1000\n",
      "CLASS 905/1000\n",
      "CLASS 906/1000\n",
      "CLASS 907/1000\n",
      "CLASS 908/1000\n",
      "CLASS 909/1000\n",
      "CLASS 910/1000\n",
      "CLASS 911/1000\n",
      "CLASS 912/1000\n",
      "CLASS 913/1000\n",
      "CLASS 914/1000\n",
      "CLASS 915/1000\n",
      "CLASS 916/1000\n",
      "CLASS 917/1000\n",
      "CLASS 918/1000\n",
      "CLASS 919/1000\n",
      "CLASS 920/1000\n",
      "CLASS 921/1000\n",
      "CLASS 922/1000\n",
      "CLASS 923/1000\n",
      "CLASS 924/1000\n",
      "CLASS 925/1000\n",
      "CLASS 926/1000\n",
      "CLASS 927/1000\n",
      "CLASS 928/1000\n",
      "CLASS 929/1000\n",
      "CLASS 930/1000\n",
      "CLASS 931/1000\n",
      "CLASS 932/1000\n",
      "CLASS 933/1000\n",
      "CLASS 934/1000\n",
      "CLASS 935/1000\n",
      "CLASS 936/1000\n",
      "CLASS 937/1000\n",
      "CLASS 938/1000\n",
      "CLASS 939/1000\n",
      "CLASS 940/1000\n",
      "CLASS 941/1000\n",
      "CLASS 942/1000\n",
      "CLASS 943/1000\n",
      "CLASS 944/1000\n",
      "CLASS 945/1000\n",
      "CLASS 946/1000\n",
      "CLASS 947/1000\n",
      "CLASS 948/1000\n",
      "CLASS 949/1000\n",
      "CLASS 950/1000\n",
      "CLASS 951/1000\n",
      "CLASS 952/1000\n",
      "CLASS 953/1000\n",
      "CLASS 954/1000\n",
      "CLASS 955/1000\n",
      "CLASS 956/1000\n",
      "CLASS 957/1000\n",
      "CLASS 958/1000\n",
      "CLASS 959/1000\n",
      "CLASS 960/1000\n",
      "CLASS 961/1000\n",
      "CLASS 962/1000\n",
      "CLASS 963/1000\n",
      "CLASS 964/1000\n",
      "CLASS 965/1000\n",
      "CLASS 966/1000\n",
      "CLASS 967/1000\n",
      "CLASS 968/1000\n",
      "CLASS 969/1000\n",
      "CLASS 970/1000\n",
      "CLASS 971/1000\n",
      "CLASS 972/1000\n",
      "CLASS 973/1000\n",
      "CLASS 974/1000\n",
      "CLASS 975/1000\n",
      "CLASS 976/1000\n",
      "CLASS 977/1000\n",
      "CLASS 978/1000\n",
      "CLASS 979/1000\n",
      "CLASS 980/1000\n",
      "CLASS 981/1000\n",
      "CLASS 982/1000\n",
      "CLASS 983/1000\n",
      "CLASS 984/1000\n",
      "CLASS 985/1000\n",
      "CLASS 986/1000\n",
      "CLASS 987/1000\n",
      "CLASS 988/1000\n",
      "CLASS 989/1000\n",
      "CLASS 990/1000\n",
      "CLASS 991/1000\n",
      "CLASS 992/1000\n",
      "CLASS 993/1000\n",
      "CLASS 994/1000\n",
      "CLASS 995/1000\n",
      "CLASS 996/1000\n",
      "CLASS 997/1000\n",
      "CLASS 998/1000\n",
      "CLASS 999/1000\n",
      "CLASS 1000/1000\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f36a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH: 48001/62500\n",
      "loss: 1.957792043685913\n",
      "\n",
      "BATCH: 49001/62500\n",
      "loss: 3.367802381515503\n",
      "\n",
      "BATCH: 50001/62500\n",
      "loss: 1.060798168182373\n",
      "\n",
      "BATCH: 51001/62500\n",
      "loss: 2.994719982147217\n",
      "\n",
      "BATCH: 52001/62500\n",
      "loss: 1.3740214109420776\n",
      "\n",
      "BATCH: 53001/62500\n",
      "loss: 1.192408800125122\n",
      "\n",
      "BATCH: 54001/62500\n",
      "loss: 2.017510414123535\n",
      "\n",
      "BATCH: 55001/62500\n",
      "loss: 2.352169990539551\n",
      "\n",
      "BATCH: 56001/62500\n",
      "loss: 1.7251794338226318\n",
      "\n",
      "BATCH: 57001/62500\n",
      "loss: 1.7635705471038818\n",
      "\n",
      "BATCH: 58001/62500\n",
      "loss: 1.9614330530166626\n",
      "\n",
      "BATCH: 59001/62500\n",
      "loss: 1.4423578977584839\n",
      "\n",
      "BATCH: 60001/62500\n",
      "loss: 1.0767019987106323\n",
      "\n",
      "BATCH: 61001/62500\n",
      "loss: 2.0136005878448486\n",
      "\n",
      "BATCH: 62001/62500\n",
      "loss: 1.711467981338501\n",
      "\n",
      "EPOCH: 6 MEAN LOSSES EPOCH: 1.7238861848899125\n",
      "Loss: 1.4690191745758057\n",
      "BATCH: 1/62500\n",
      "loss: 0.9097316265106201\n",
      "\n",
      "BATCH: 1001/62500\n",
      "loss: 1.5583593845367432\n",
      "\n",
      "BATCH: 2001/62500\n",
      "loss: 0.639952540397644\n",
      "\n",
      "BATCH: 3001/62500\n",
      "loss: 0.434953898191452\n",
      "\n",
      "BATCH: 4001/62500\n",
      "loss: 1.943143367767334\n",
      "\n",
      "BATCH: 5001/62500\n",
      "loss: 0.45002636313438416\n",
      "\n",
      "BATCH: 6001/62500\n",
      "loss: 1.1837095022201538\n",
      "\n",
      "BATCH: 7001/62500\n",
      "loss: 1.2580019235610962\n",
      "\n",
      "BATCH: 8001/62500\n",
      "loss: 0.8231417536735535\n",
      "\n",
      "BATCH: 9001/62500\n",
      "loss: 0.8749845027923584\n",
      "\n",
      "BATCH: 10001/62500\n",
      "loss: 2.9408302307128906\n",
      "\n",
      "BATCH: 11001/62500\n",
      "loss: 1.1128392219543457\n",
      "\n",
      "BATCH: 12001/62500\n",
      "loss: 0.899516224861145\n",
      "\n",
      "BATCH: 13001/62500\n",
      "loss: 2.0564212799072266\n",
      "\n",
      "BATCH: 14001/62500\n",
      "loss: 1.3828977346420288\n",
      "\n",
      "BATCH: 15001/62500\n",
      "loss: 1.2121837139129639\n",
      "\n",
      "BATCH: 16001/62500\n",
      "loss: 1.5344576835632324\n",
      "\n",
      "BATCH: 17001/62500\n",
      "loss: 1.522609829902649\n",
      "\n",
      "BATCH: 18001/62500\n",
      "loss: 1.249752163887024\n",
      "\n",
      "BATCH: 19001/62500\n",
      "loss: 0.6593955755233765\n",
      "\n",
      "BATCH: 20001/62500\n",
      "loss: 1.9104607105255127\n",
      "\n",
      "BATCH: 21001/62500\n",
      "loss: 1.867018699645996\n",
      "\n",
      "BATCH: 22001/62500\n",
      "loss: 0.7011687159538269\n",
      "\n",
      "BATCH: 23001/62500\n",
      "loss: 0.6060234904289246\n",
      "\n",
      "BATCH: 24001/62500\n",
      "loss: 1.4087005853652954\n",
      "\n",
      "BATCH: 25001/62500\n",
      "loss: 2.615882158279419\n",
      "\n",
      "BATCH: 26001/62500\n",
      "loss: 1.0793516635894775\n",
      "\n",
      "BATCH: 27001/62500\n",
      "loss: 1.613014578819275\n",
      "\n",
      "BATCH: 28001/62500\n",
      "loss: 0.8042539358139038\n",
      "\n",
      "BATCH: 29001/62500\n",
      "loss: 1.5829315185546875\n",
      "\n",
      "BATCH: 30001/62500\n",
      "loss: 2.288951873779297\n",
      "\n",
      "BATCH: 31001/62500\n",
      "loss: 1.7588778734207153\n",
      "\n",
      "BATCH: 32001/62500\n",
      "loss: 1.6422760486602783\n",
      "\n",
      "BATCH: 33001/62500\n",
      "loss: 1.4314711093902588\n",
      "\n",
      "BATCH: 34001/62500\n",
      "loss: 1.4434006214141846\n",
      "\n",
      "BATCH: 35001/62500\n",
      "loss: 1.3639202117919922\n",
      "\n",
      "BATCH: 36001/62500\n",
      "loss: 2.020106792449951\n",
      "\n",
      "BATCH: 37001/62500\n",
      "loss: 0.9103597402572632\n",
      "\n",
      "BATCH: 38001/62500\n",
      "loss: 1.8512592315673828\n",
      "\n",
      "BATCH: 39001/62500\n",
      "loss: 1.4622583389282227\n",
      "\n",
      "BATCH: 40001/62500\n",
      "loss: 2.003207206726074\n",
      "\n",
      "BATCH: 41001/62500\n",
      "loss: 2.6169025897979736\n",
      "\n",
      "BATCH: 42001/62500\n",
      "loss: 2.689758539199829\n",
      "\n",
      "BATCH: 43001/62500\n",
      "loss: 2.1186039447784424\n",
      "\n",
      "BATCH: 44001/62500\n",
      "loss: 1.2288963794708252\n",
      "\n",
      "BATCH: 45001/62500\n",
      "loss: 2.7774462699890137\n",
      "\n",
      "BATCH: 46001/62500\n",
      "loss: 1.4099639654159546\n",
      "\n",
      "BATCH: 47001/62500\n",
      "loss: 0.3488610088825226\n",
      "\n",
      "BATCH: 48001/62500\n",
      "loss: 2.290818691253662\n",
      "\n",
      "BATCH: 49001/62500\n",
      "loss: 0.89166659116745\n",
      "\n",
      "BATCH: 50001/62500\n",
      "loss: 2.2341108322143555\n",
      "\n",
      "BATCH: 51001/62500\n",
      "loss: 0.9640679955482483\n",
      "\n",
      "BATCH: 52001/62500\n",
      "loss: 1.4922980070114136\n",
      "\n",
      "BATCH: 53001/62500\n",
      "loss: 1.2763314247131348\n",
      "\n",
      "BATCH: 54001/62500\n",
      "loss: 0.8723785281181335\n",
      "\n",
      "BATCH: 55001/62500\n",
      "loss: 1.6133475303649902\n",
      "\n",
      "BATCH: 56001/62500\n",
      "loss: 1.3622119426727295\n",
      "\n",
      "BATCH: 57001/62500\n",
      "loss: 0.8584213256835938\n",
      "\n",
      "BATCH: 58001/62500\n",
      "loss: 1.5661693811416626\n",
      "\n",
      "BATCH: 59001/62500\n",
      "loss: 2.101163625717163\n",
      "\n",
      "BATCH: 60001/62500\n",
      "loss: 1.8847482204437256\n",
      "\n",
      "BATCH: 61001/62500\n",
      "loss: 1.610410213470459\n",
      "\n",
      "BATCH: 62001/62500\n",
      "loss: 0.6120561361312866\n",
      "\n",
      "EPOCH: 7 MEAN LOSSES EPOCH: 1.5339390450069605\n",
      "Loss: 1.3740029335021973\n",
      "BATCH: 1/62500\n",
      "loss: 1.965444564819336\n",
      "\n",
      "BATCH: 1001/62500\n",
      "loss: 1.458118200302124\n",
      "\n",
      "BATCH: 2001/62500\n",
      "loss: 0.5843273401260376\n",
      "\n",
      "BATCH: 3001/62500\n",
      "loss: 1.5203007459640503\n",
      "\n",
      "BATCH: 4001/62500\n",
      "loss: 1.0793554782867432\n",
      "\n",
      "BATCH: 5001/62500\n",
      "loss: 2.2788829803466797\n",
      "\n",
      "BATCH: 6001/62500\n",
      "loss: 1.1715461015701294\n",
      "\n",
      "BATCH: 7001/62500\n",
      "loss: 1.3711730241775513\n",
      "\n",
      "BATCH: 8001/62500\n",
      "loss: 1.4741193056106567\n",
      "\n",
      "BATCH: 9001/62500\n",
      "loss: 2.2156245708465576\n",
      "\n",
      "BATCH: 10001/62500\n",
      "loss: 0.12213156372308731\n",
      "\n",
      "BATCH: 11001/62500\n",
      "loss: 1.3328144550323486\n",
      "\n",
      "BATCH: 12001/62500\n",
      "loss: 0.4906204640865326\n",
      "\n",
      "BATCH: 13001/62500\n",
      "loss: 1.9826791286468506\n",
      "\n",
      "BATCH: 14001/62500\n",
      "loss: 0.9899751543998718\n",
      "\n",
      "BATCH: 15001/62500\n",
      "loss: 0.8796677589416504\n",
      "\n",
      "BATCH: 16001/62500\n",
      "loss: 3.669590711593628\n",
      "\n",
      "BATCH: 17001/62500\n",
      "loss: 0.9995290637016296\n",
      "\n",
      "BATCH: 18001/62500\n",
      "loss: 1.3179442882537842\n",
      "\n",
      "BATCH: 19001/62500\n",
      "loss: 1.7659971714019775\n",
      "\n",
      "BATCH: 20001/62500\n",
      "loss: 1.1767289638519287\n",
      "\n",
      "BATCH: 21001/62500\n",
      "loss: 0.50771564245224\n",
      "\n",
      "BATCH: 22001/62500\n",
      "loss: 1.8595292568206787\n",
      "\n",
      "BATCH: 23001/62500\n",
      "loss: 1.891029715538025\n",
      "\n",
      "BATCH: 24001/62500\n",
      "loss: 3.412862539291382\n",
      "\n",
      "BATCH: 25001/62500\n",
      "loss: 0.36339399218559265\n",
      "\n",
      "BATCH: 26001/62500\n",
      "loss: 1.7617660760879517\n",
      "\n",
      "BATCH: 27001/62500\n",
      "loss: 1.7363992929458618\n",
      "\n",
      "BATCH: 28001/62500\n",
      "loss: 1.8365607261657715\n",
      "\n",
      "BATCH: 29001/62500\n",
      "loss: 1.1622426509857178\n",
      "\n",
      "BATCH: 30001/62500\n",
      "loss: 2.7406439781188965\n",
      "\n",
      "BATCH: 31001/62500\n",
      "loss: 2.0284547805786133\n",
      "\n",
      "BATCH: 32001/62500\n",
      "loss: 1.4118151664733887\n",
      "\n",
      "BATCH: 33001/62500\n",
      "loss: 0.5357161164283752\n",
      "\n",
      "BATCH: 34001/62500\n",
      "loss: 1.472253441810608\n",
      "\n",
      "BATCH: 35001/62500\n",
      "loss: 1.6209356784820557\n",
      "\n",
      "BATCH: 36001/62500\n",
      "loss: 0.5846750140190125\n",
      "\n",
      "BATCH: 37001/62500\n",
      "loss: 3.0350911617279053\n",
      "\n",
      "BATCH: 38001/62500\n",
      "loss: 1.9029227495193481\n",
      "\n",
      "BATCH: 39001/62500\n",
      "loss: 0.41461774706840515\n",
      "\n",
      "BATCH: 40001/62500\n",
      "loss: 1.6225377321243286\n",
      "\n",
      "BATCH: 41001/62500\n",
      "loss: 2.4477529525756836\n",
      "\n",
      "BATCH: 42001/62500\n",
      "loss: 1.4490643739700317\n",
      "\n",
      "BATCH: 43001/62500\n",
      "loss: 0.901432991027832\n",
      "\n",
      "BATCH: 44001/62500\n",
      "loss: 1.3175454139709473\n",
      "\n",
      "BATCH: 45001/62500\n",
      "loss: 0.2644750773906708\n",
      "\n",
      "BATCH: 46001/62500\n",
      "loss: 1.4608101844787598\n",
      "\n",
      "BATCH: 47001/62500\n",
      "loss: 1.2929799556732178\n",
      "\n",
      "BATCH: 48001/62500\n",
      "loss: 0.8549495339393616\n",
      "\n",
      "BATCH: 49001/62500\n",
      "loss: 1.1302186250686646\n",
      "\n",
      "BATCH: 50001/62500\n",
      "loss: 1.14601731300354\n",
      "\n",
      "BATCH: 51001/62500\n",
      "loss: 2.512664318084717\n",
      "\n",
      "BATCH: 52001/62500\n",
      "loss: 0.40035995841026306\n",
      "\n",
      "BATCH: 53001/62500\n",
      "loss: 2.3823699951171875\n",
      "\n",
      "BATCH: 54001/62500\n",
      "loss: 1.1446263790130615\n",
      "\n",
      "BATCH: 55001/62500\n",
      "loss: 0.9521244168281555\n",
      "\n",
      "BATCH: 56001/62500\n",
      "loss: 1.663417100906372\n",
      "\n",
      "BATCH: 57001/62500\n",
      "loss: 0.7335624694824219\n",
      "\n",
      "BATCH: 58001/62500\n",
      "loss: 0.7250325679779053\n",
      "\n",
      "BATCH: 59001/62500\n",
      "loss: 2.305900812149048\n",
      "\n",
      "BATCH: 60001/62500\n",
      "loss: 2.5376715660095215\n",
      "\n",
      "BATCH: 61001/62500\n",
      "loss: 0.9026867151260376\n",
      "\n",
      "BATCH: 62001/62500\n",
      "loss: 1.1943281888961792\n",
      "\n",
      "EPOCH: 8 MEAN LOSSES EPOCH: 1.3579337318145632\n",
      "Loss: 1.2865016460418701\n",
      "BATCH: 1/62500\n",
      "loss: 1.4649392366409302\n",
      "\n",
      "BATCH: 1001/62500\n",
      "loss: 1.2375761270523071\n",
      "\n",
      "BATCH: 2001/62500\n",
      "loss: 0.9870894551277161\n",
      "\n",
      "BATCH: 3001/62500\n",
      "loss: 1.1353999376296997\n",
      "\n",
      "BATCH: 4001/62500\n",
      "loss: 0.8462402820587158\n",
      "\n",
      "BATCH: 5001/62500\n",
      "loss: 0.9192960262298584\n",
      "\n",
      "BATCH: 6001/62500\n",
      "loss: 1.1937146186828613\n",
      "\n",
      "BATCH: 7001/62500\n",
      "loss: 0.6746516227722168\n",
      "\n",
      "BATCH: 8001/62500\n",
      "loss: 0.1361374855041504\n",
      "\n",
      "BATCH: 9001/62500\n",
      "loss: 0.33585718274116516\n",
      "\n",
      "BATCH: 10001/62500\n",
      "loss: 1.8684834241867065\n",
      "\n",
      "BATCH: 11001/62500\n",
      "loss: 1.3536752462387085\n",
      "\n",
      "BATCH: 12001/62500\n",
      "loss: 0.22172234952449799\n",
      "\n",
      "BATCH: 13001/62500\n",
      "loss: 1.3019459247589111\n",
      "\n",
      "BATCH: 14001/62500\n",
      "loss: 0.6160862445831299\n",
      "\n",
      "BATCH: 15001/62500\n",
      "loss: 1.0526642799377441\n",
      "\n",
      "BATCH: 16001/62500\n",
      "loss: 0.9173036813735962\n",
      "\n",
      "BATCH: 17001/62500\n",
      "loss: 1.4664266109466553\n",
      "\n",
      "BATCH: 18001/62500\n",
      "loss: 3.3457541465759277\n",
      "\n",
      "BATCH: 19001/62500\n",
      "loss: 0.5018196702003479\n",
      "\n",
      "BATCH: 20001/62500\n",
      "loss: 1.2247921228408813\n",
      "\n",
      "BATCH: 21001/62500\n",
      "loss: 0.6162180304527283\n",
      "\n",
      "BATCH: 22001/62500\n",
      "loss: 0.9205268025398254\n",
      "\n",
      "BATCH: 23001/62500\n",
      "loss: 1.6826368570327759\n",
      "\n",
      "BATCH: 24001/62500\n",
      "loss: 0.950371503829956\n",
      "\n",
      "BATCH: 25001/62500\n",
      "loss: 1.1570143699645996\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 23\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#print(f\"EPOCH: {i} BATCH: {n_batch}/{len(train_loader)} LOSS: {loss.item()}\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check loss at near 220 epochs\n",
    "# 8 epochs, loss = 1.3579337318145632\n",
    "#darknet = darknet53(darknet_architecture, classes).to(device)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(params=darknet.parameters(), lr=lr)\n",
    "i=1\n",
    "\n",
    "best_parameters = None\n",
    "best_loss = math.inf\n",
    "while(True):\n",
    "    losses = []\n",
    "    n_batch = 0\n",
    "    train_loader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "    total_batches = len(train_loader)\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = darknet(x)\n",
    "        loss = ce_loss(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f\"EPOCH: {i} BATCH: {n_batch}/{len(train_loader)} LOSS: {loss.item()}\")\n",
    "        if n_batch % 1000 == 0:\n",
    "            print(f\"BATCH: {n_batch+1}/{total_batches}\")\n",
    "            print(f\"loss: {loss.item()}\\n\")\n",
    "        n_batch+=1\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    mean_losses = sum(losses)/len(losses)\n",
    "    if mean_losses < best_loss:\n",
    "        if best_loss != math.inf:\n",
    "            os.remove(f\"../backup_models/darknet_weights_loss_{best_loss}\")\n",
    "        best_loss = mean_losses\n",
    "        best_parameters = darknet.state_dict()\n",
    "        torch.save(best_parameters, f\"../backup_models/darknet_weights_loss_{best_loss}\")\n",
    "    print(f\"EPOCH: {i} MEAN LOSSES EPOCH: {sum(losses)/len(losses)}\")\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(testset)\n",
    "print(f\"N :{N}\")\n",
    "top1 = 0\n",
    "top5 = 0\n",
    "i=1\n",
    "with torch.no_grad():\n",
    "    batch_size = 8\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size)\n",
    "    N_test_loader = len(test_loader)\n",
    "    for x, y in test_loader:\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        pred = darknet(x)\n",
    "        \n",
    "        for idx_batch in range(x.shape[0]):\n",
    "            top1_pred = torch.argmax(pred[idx_batch])\n",
    "            if top1_pred == y[idx_batch]:\n",
    "                top1 += 1\n",
    "                top5 += 1\n",
    "                continue\n",
    "            \n",
    "            top5_pred = torch.topk(pred[idx_batch], 5).indices\n",
    "            if y[idx_batch] in top5_pred:\n",
    "                top5 += 1\n",
    "        if i%1000 == 0:\n",
    "            print(f\"{i}/{N_test_loader}\")\n",
    "        i+=1\n",
    "        #print(f\"TOP1: {top1}\")\n",
    "        #print(f\"TOP5: {top5}\")\n",
    "print(f\"ACCURACY TOP1: {top1/N}\")\n",
    "print(f\"ACCURACY TOP5: {top5/N}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

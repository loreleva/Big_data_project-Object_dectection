{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d589d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5a8c9",
   "metadata": {},
   "source": [
    "# Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0054781",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_architecture = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"residual\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"residual\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"residualYolo\", 8],\n",
    "    # first yolo route\n",
    "    (512, 3, 2),\n",
    "    [\"residualYolo\", 8],\n",
    "    # second yolo route\n",
    "    (1024, 3, 2),\n",
    "    [\"residual\", 4],\n",
    "    # third yolo route\n",
    "    [\"yolo\", 1024],\n",
    "    [\"yolo\", 512],\n",
    "    [\"yolo\", 256],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3796b",
   "metadata": {},
   "source": [
    "# Blocks Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        padding=1 if kernel_size == 3 else 0\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              bias=not bn_act,\n",
    "                              padding=padding, \n",
    "                              **kwargs)\n",
    "        # if batchnorm, then leaky relu is the activation function\n",
    "        self.use_bn_act = bn_act\n",
    "        if self.use_bn_act:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "            self.leaky = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block of convolutional layers\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    ConvLayer(channels, channels // 2, kernel_size=1),\n",
    "                    ConvLayer(channels // 2, channels, kernel_size=3),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x) + self.use_residual * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26718d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block of convolutional layers with feature map to be saved for detections\n",
    "class ConvBlockYolo(nn.Module):\n",
    "    def __init__(self, channels, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.feat_map = None\n",
    "        num_layer = 1\n",
    "        for i in range(num_repeats):\n",
    "            if num_layer == 5:\n",
    "                self.layers += [\n",
    "                    ConvLayer(channels, channels // 2, kernel_size=1),\n",
    "                    ConvLayer(channels // 2, channels, kernel_size=3)\n",
    "                ]\n",
    "            else:\n",
    "                self.layers += [\n",
    "                    nn.Sequential(\n",
    "                        ConvLayer(channels, channels // 2, kernel_size=1),\n",
    "                        ConvLayer(channels // 2, channels, kernel_size=3),\n",
    "                    )\n",
    "                ]\n",
    "            num_layer += 2\n",
    "            \n",
    "    def forward(self, x):\n",
    "        self.feat_map = None\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ConvLayer):\n",
    "                if self.feat_map == None:\n",
    "                    self.feat_map = layer(x)\n",
    "                else:\n",
    "                    x = layer(self.feat_map) + x\n",
    "            else:\n",
    "                x = layer(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detection block which outputs bboxes for each grid cell\n",
    "class DetectionBlock(nn.Module):\n",
    "    def __init__(self, channels, num_classes):\n",
    "        super().__init__()\n",
    "        # feature map to be passed to the the next yolo prediction block\n",
    "        self.feat_map = None\n",
    "        # conv layers\n",
    "        self.convBlock = ConvBlock(channels, use_residual=False, num_repeats=2)\n",
    "        # penultimate conv block, its feature map must be saved to be passed to other detections block\n",
    "        self.penultConvBlock = ConvLayer(channels, channels // 2, kernel_size=1)\n",
    "        self.ultConvBlock = ConvLayer(channels // 2, channels, kernel_size=3)\n",
    "        # output of last convolution is a tensor: \n",
    "        # batch_size x \n",
    "        # predictions_size (= n_anchor_boxes * (bbox_coord + obj + n_classes) ) x\n",
    "        # grid_size x grid_size (13 small objects, 26 medium objects, 52 big objects)\n",
    "        self.outputPredictions = ConvLayer(channels, 3 * (4+1+num_classes), kernel_size=1, bn_act=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convBlock(x)\n",
    "        self.feat_map = self.penultConvBlock(x)\n",
    "        x = self.ultConvBlock(self.feat_map)\n",
    "        x = self.outputPredictions(x)\n",
    "        # change order of dimensions in: batch_size x grid_size x grid_size x predictions_size\n",
    "        x = torch.permute(x, (0, 2, 3, 1))\n",
    "        x_shape = x.shape\n",
    "        # reshape output to: batch_size x grid_size x grid_size x num_anchor_boxes_per_cell x (bbox_coord + obj + n_classes)  \n",
    "        return x.reshape(x_shape[0], x_shape[1], x_shape[2], 3,  -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f5b1e",
   "metadata": {},
   "source": [
    "# Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefb7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class yolov3(nn.Module):\n",
    "    def __init__(self, architecture, num_classes, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_layers(architecture)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        num_detection = 0\n",
    "        # list of feature maps values needed for yolo detection\n",
    "        feat_maps = []\n",
    "        # tensor of the detections, ordered from the biggest detections to the smallest\n",
    "        detections = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            # if conv block of darknet, save feature maps used later for detection\n",
    "            if isinstance(layer, ConvBlockYolo):\n",
    "                feat_maps.append(layer.feat_map)\n",
    "                \n",
    "            # if detection block, save detection and set x to value needed for the next detection\n",
    "            elif isinstance(layer, DetectionBlock):\n",
    "                # x is a tensor of dim: n_anchor_boxes * (bbox_coord + obj + n_classes)\n",
    "                detections.append(x)\n",
    "                x = layer.feat_map\n",
    "                num_detection += 1\n",
    "            # concat darknet feat map with upsampled tensor of previous detection\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                # feat maps from darkent are ordered from smallest to biggest\n",
    "                # yolo detection uses feat maps from biggest to smallest\n",
    "                # then we take darknet feat maps backwards\n",
    "                x = torch.cat((feat_maps[-num_detection], x), dim=1)\n",
    "        # return list of detections from the biggest to the smallest\n",
    "        # each detection is a tensor\n",
    "        return detections\n",
    "        \n",
    "    def _create_layers(self, architecture):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "        num_detection = 1\n",
    "        for module in architecture:\n",
    "            # conv layer\n",
    "            if isinstance(module, tuple):\n",
    "                if module[-1] == \"linear\":\n",
    "                    bn_act = False\n",
    "                else:\n",
    "                    bn_act = True\n",
    "                layers.append(ConvLayer(in_channels, \n",
    "                                        module[0], \n",
    "                                        kernel_size=module[1],\n",
    "                                        bn_act=bn_act,\n",
    "                                        stride=module[2])\n",
    "                             )\n",
    "                in_channels = module[0]\n",
    "                continue\n",
    "            \n",
    "            if isinstance(module, list):\n",
    "                # residual block\n",
    "                if module[0] == \"residual\":\n",
    "                    layers.append(ConvBlock(in_channels,\n",
    "                                            num_repeats=module[1])\n",
    "                                 )\n",
    "                    continue\n",
    "                # residual block with feature map to be saved for detection\n",
    "                elif module[0] == \"residualYolo\":\n",
    "                    layers.append(ConvBlockYolo(in_channels,\n",
    "                                            num_repeats=module[1])\n",
    "                                 )\n",
    "                    continue\n",
    "                # detection Block\n",
    "                elif module[0] == \"yolo\":\n",
    "                    if num_detection == 3:\n",
    "                        layers.append(DetectionBlock(module[1],\n",
    "                                                     self.num_classes)\n",
    "                                     )\n",
    "                    else:\n",
    "                        layers += [DetectionBlock(module[1], \n",
    "                                                  self.num_classes),\n",
    "                                   ConvLayer(module[1] // 2, \n",
    "                                             module[1] // 4,\n",
    "                                             kernel_size=1),\n",
    "                                   nn.Upsample(scale_factor=2)\n",
    "                                  ]\n",
    "                    num_detection += 1\n",
    "                    continue\n",
    "                    \n",
    "        return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686dc04",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuimages import NuImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuim = NuImages(dataroot='/home/loreleva/Desktop/Big Data Project/data/sets/nuimages', version='v1.0-train', verbose=True, lazy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0875e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUIMAGES_LABELS = [\n",
    "    \"human\",\n",
    "    \"barrier\",\n",
    "    \"trafficcone\",\n",
    "    \"bicycle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"truck\",\n",
    "    \"construction_vehicles\",\n",
    "    \"trailer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_category = {\n",
    " \"animal\" : None,\n",
    " \"human.pedestrian.adult\" : \"human\",\n",
    " \"human.pedestrian.child\" : \"human\",\n",
    " \"human.pedestrian.construction_worker\" : \"human\",\n",
    " \"human.pedestrian.personal_mobility\" : \"human\",\n",
    " \"human.pedestrian.police_officer\" : \"human\",\n",
    " \"human.pedestrian.stroller\" : \"human\",\n",
    " \"human.pedestrian.wheelchair\" : \"human\",\n",
    " \"movable_object.barrier\" : \"barrier\",\n",
    " \"movable_object.debris\" : None,\n",
    " \"movable_object.pushable_pullable\" : None,\n",
    " \"movable_object.trafficcone\" : \"trafficcone\",\n",
    " \"static_object.bicycle_rack\" : None,\n",
    " \"vehicle.bicycle\" : \"bicycle\",\n",
    " \"vehicle.bus.bendy\" : \"bus\",\n",
    " \"vehicle.bus.rigid\" : \"bus\",\n",
    " \"vehicle.car\" : \"car\",\n",
    " \"vehicle.ego\" : None,\n",
    " \"vehicle.construction\" : \"construction_vehicles\",\n",
    " \"vehicle.emergency.ambulance\" : \"truck\",\n",
    " \"vehicle.emergency.police\" : \"car\",\n",
    " \"vehicle.motorcycle\" : \"motorcycle\",\n",
    " \"vehicle.trailer\" : \"trailer\",\n",
    " \"vehicle.truck\" : \"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxes(sample_token):\n",
    "    # returns list of annotations: [[bbox, category], ...]\n",
    "    ann_tokens, _ = nuim.list_anns(sample_token, verbose=False)\n",
    "    annotations = []\n",
    "    for token in ann_tokens:\n",
    "        object_ann = nuim.get(\"object_ann\", token)\n",
    "        category = selection_category[nuim.get(\"category\", object_ann[\"category_token\"])[\"name\"]]\n",
    "        if category == None:\n",
    "            continue\n",
    "        annotations.append(object_ann[\"bbox\"] + [category])\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_bboxes(img_h, img_w, bboxes, confidence=False):\n",
    "    # transform elements of bboxes in range [0,1]\n",
    "    new_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        if confidence:\n",
    "            new_bboxes.append([bbox[0]/img_w, bbox[1]/img_h, bbox[2]/img_w, bbox[3]/img_h, bbox[4], bbox[5]])\n",
    "        else:\n",
    "            new_bboxes.append([bbox[0]/img_w, bbox[1]/img_h, bbox[2]/img_w, bbox[3]/img_h, bbox[4]])\n",
    "    return new_bboxes\n",
    "\n",
    "def unnormalize_bboxes(img_h, img_w, bboxes, confidence=False):\n",
    "    # transform elements of bboxes from range [0,1] in range of integers [0, img_w], [0, img_h]\n",
    "    new_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        if confidence:\n",
    "            new_bboxes.append([int(bbox[0]*img_w), int(bbox[1]*img_h), int(bbox[2]*img_w), int(bbox[3]*img_h), bbox[4], bbox[5]])\n",
    "        else:\n",
    "            new_bboxes.append([int(bbox[0]*img_w), int(bbox[1]*img_h), int(bbox[2]*img_w), int(bbox[3]*img_h), bbox[4]])\n",
    "    return new_bboxes\n",
    "    \n",
    "def pascal_to_yolo(img_h, img_w, bboxes, confidence=False):\n",
    "    # convert bboxes from pascal notation to yolo notation\n",
    "    # i.e., from [xmin, ymin, xmax, ymax, category] notation to [xcenter, ycenter, width, height, category] normalized\n",
    "    new_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        # check if bbox exceed image bounds\n",
    "        if bbox[2] > img_w:\n",
    "            bbox[2] = img_w\n",
    "        if bbox[3] > img_h:\n",
    "            bbox[3] = img_h\n",
    "        bbox_h = bbox[3] - bbox[1]\n",
    "        bbox_w = bbox[2] - bbox[0]\n",
    "        if bbox_h == 0 or bbox_w == 0:\n",
    "            continue\n",
    "        if confidence:\n",
    "            new_bboxes.append([bbox[0] + bbox_w/2, \n",
    "                           bbox[1] + bbox_h/2, \n",
    "                           bbox_w,\n",
    "                           bbox_h, \n",
    "                           bbox[4], \n",
    "                           bbox[5]\n",
    "                          ])\n",
    "        else:\n",
    "            new_bboxes.append([bbox[0] + bbox_w/2, \n",
    "                           bbox[1] + bbox_h/2, \n",
    "                           bbox_w,\n",
    "                           bbox_h, \n",
    "                           bbox[4]\n",
    "                          ])\n",
    "    return normalize_bboxes(img_h, img_w, new_bboxes, confidence=confidence)\n",
    "    \n",
    "def yolo_to_pascal(img_h, img_w, bboxes, confidence=False):\n",
    "    # convert bboxes from yolo notation to pascal notation\n",
    "    # i.e., from [xcenter, ycenter, width, height, category] notation to [xmin, ymin, xmax, ymax, category] notation\n",
    "    new_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        if bbox[2] == 0 or bbox[3] == 0:\n",
    "            continue\n",
    "        if confidence:\n",
    "            new_bboxes.append([bbox[0] - bbox[2]/2, \n",
    "                           bbox[1] - bbox[3]/2, \n",
    "                           bbox[0] + bbox[2]/2,\n",
    "                           bbox[1] + bbox[3]/2, \n",
    "                           bbox[4],\n",
    "                           bbox[5]\n",
    "                          ])\n",
    "        else:\n",
    "            new_bboxes.append([bbox[0] - bbox[2]/2, \n",
    "                           bbox[1] - bbox[3]/2, \n",
    "                           bbox[0] + bbox[2]/2,\n",
    "                           bbox[1] + bbox[3]/2, \n",
    "                           bbox[4]\n",
    "                          ])\n",
    "    return unnormalize_bboxes(img_h, img_w, new_bboxes, confidence=confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = A.Compose(\n",
    "    [\n",
    "        A.Resize(416, 416)\n",
    "\n",
    "    ], bbox_params=A.BboxParams(format=\"yolo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(nuim, path_nuimages, type_dataset, path_dataset, resize):\n",
    "    path_nuimages_samples = os.path.join(path_nuimages, \"samples\")\n",
    "    samples_cam_names = os.listdir(path_nuimages_samples)\n",
    "    # create directories if not exist\n",
    "    if not os.path.exists(path_dataset):\n",
    "        os.mkdir(path_dataset)\n",
    "    path_type_dataset = os.path.join(path_dataset, type_dataset)\n",
    "    if not os.path.exists(path_type_dataset):\n",
    "        os.mkdir(path_type_dataset)\n",
    "    path_samples = os.path.join(path_type_dataset, \"samples\")\n",
    "    if not os.path.exists(path_samples):\n",
    "        os.mkdir(path_samples)\n",
    "    path_bbox_ann = os.path.join(path_type_dataset, \"bbox_annotations\")\n",
    "    if not os.path.exists(path_bbox_ann):\n",
    "        os.mkdir(path_bbox_ann)\n",
    "    for cam_name in samples_cam_names:\n",
    "        path_cam_sample = os.path.join(path_samples, cam_name)\n",
    "        if not os.path.exists(path_cam_sample):\n",
    "            os.mkdir(path_cam_sample)\n",
    "        path_cam_annotations = os.path.join(path_bbox_ann, cam_name)\n",
    "        if not os.path.exists(path_cam_annotations):\n",
    "            os.mkdir(path_cam_annotations)\n",
    "    i=0\n",
    "    N = len(nuim.sample)\n",
    "    i = 1\n",
    "    \n",
    "    main_csv = \"filename_img; filename_annotations\\n\"\n",
    "    for sample in nuim.sample:\n",
    "        print(f\"SAMPLE: {i}/{N}\")\n",
    "        i+=1\n",
    "        bboxes = get_bboxes(sample[\"token\"])\n",
    "        filename_img = nuim.get(\"sample_data\", sample[\"key_camera_token\"])[\"filename\"]\n",
    "        np_img = np.array(Image.open(os.path.join(path_nuimages, filename_img)).convert(\"RGB\"))\n",
    "        bboxes = pascal_to_yolo(np_img.shape[0], np_img.shape[1], bboxes)\n",
    "        resize_img = resize(image=np_img, bboxes=bboxes)\n",
    "        new_img = resize_img[\"image\"]\n",
    "        bboxes = resize_img[\"bboxes\"]\n",
    "\n",
    "        plt.imsave(os.path.join(path_type_dataset, filename_img), new_img.astype(np.uint8))\n",
    "        filename_bbox_ann = \"\"\n",
    "        if len(bboxes) != 0:\n",
    "            bboxes_csv = \"xcenter; ycenter; w; h; class\\n\"\n",
    "            for bbox in bboxes:\n",
    "                bboxes_csv = bboxes_csv + f\"{bbox[0]}; {bbox[1]}; {bbox[2]}; {bbox[3]}; {NUIMAGES_LABELS.index(bbox[4])}\\n\"\n",
    "            filename_bbox_ann = filename_img.split(\".\")[0] + \".csv\"\n",
    "            filename_bbox_ann = \"bbox_annotations\" + filename_bbox_ann[7:]\n",
    "            with open(os.path.join(path_type_dataset, filename_bbox_ann), \"w\") as f:\n",
    "                f.write(bboxes_csv)\n",
    "                f.close()\n",
    "        main_csv = main_csv + f\"{filename_img}; {filename_bbox_ann}\\n\"\n",
    "    with open(os.path.join(path_type_dataset, \"main.csv\"), \"w\") as f:\n",
    "        f.write(main_csv)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daca246",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea89f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1, bbox2, img_h=416, img_w=416, confidence=True):\n",
    "    # transform boxes in [xmin, ymin, xmax, ymax]\n",
    "    bbox1 = yolo_to_pascal(img_h, img_w, [bbox1], confidence=confidence)[0]\n",
    "    bbox2 = yolo_to_pascal(img_h, img_w, [bbox2], confidence=confidence)[0]\n",
    "    \n",
    "    xA = max(bbox1[0], bbox2[0])\n",
    "    yA = max(bbox1[1], bbox2[1])\n",
    "    xB = min(bbox1[2], bbox2[2])\n",
    "    yB = min(bbox1[3], bbox2[3])\n",
    "    \n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    \n",
    "    box1Area = (bbox1[2] - bbox1[0] + 1) * (bbox1[3] - bbox1[1] + 1)\n",
    "    box2Area = (bbox2[2] - bbox2[0] + 1) * (bbox2[3] - bbox2[1] + 1)\n",
    "    \n",
    "    iou = interArea / float(box1Area + box2Area - interArea)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + 1 / np.exp(x))\n",
    "\n",
    "def inverse_sigmoid(x):\n",
    "    return np.log(x/(1-x))\n",
    "\n",
    "def get_tx(b_x, c_x):\n",
    "    return inverse_sigmoid(b_x - c_x)\n",
    "\n",
    "def get_ty(b_y, c_y):\n",
    "    return inverse_sigmoid(b_y - c_y)\n",
    "\n",
    "def get_bx(t_x, c_x, cell_size):\n",
    "    return sigmoid(t_x)*cell_size + c_x\n",
    "\n",
    "def get_by(t_y, c_y, cell_size):\n",
    "    return sigmoid(t_y)*cell_size + c_y\n",
    "\n",
    "def get_bw(p_w, t_w):\n",
    "    return p_w * np.exp(t_w)\n",
    "\n",
    "def get_bh(p_h, t_h):\n",
    "    return p_h * np.exp(t_h)\n",
    "\n",
    "def get_tw(b_w, p_w):\n",
    "    return np.log(b_w/p_w)\n",
    "\n",
    "def get_th(b_h, p_h):\n",
    "    return np.log(b_h/p_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd831e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainset(Dataset):\n",
    "    def __init__(self, path_trainset, anchors):\n",
    "        self.path_trainset = path_trainset\n",
    "        self.main_df = pd.read_csv(os.path.join(self.path_trainset, \"main.csv\"),\n",
    "                                   sep=\";\", \n",
    "                                   skipinitialspace=True, \n",
    "                                   na_filter=False)\n",
    "        self.transform = transforms.PILToTensor()\n",
    "        self.anchors = anchors\n",
    "        self.S = [13, 26, 52]\n",
    "        self.thr_ignore = 0.5\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.main_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # retrieve ground truth bboxes\n",
    "        row = self.main_df.iloc[idx]\n",
    "        bboxes = []\n",
    "        if row[\"filename_annotations\"] != \"\":\n",
    "            # load bboxes\n",
    "            bbox_df = pd.read_csv(os.path.join(self.path_trainset, row[\"filename_annotations\"]), \n",
    "                              sep=\";\", \n",
    "                              skipinitialspace=True, \n",
    "                              na_filter=False)\n",
    "            for i in range(len(bbox_df)):\n",
    "                # insert objectness to bbox to compatiblity with anchor boxes\n",
    "                bbox = bbox_df.iloc[i].tolist()\n",
    "                bbox.insert(4, 1)\n",
    "                bboxes.append(bbox)\n",
    "                \n",
    "        # create anchor bboxes\n",
    "        # list of indices, where each idx is associated to a ground truth bbox\n",
    "        # and its value is the iou and the index of the associated anchor box\n",
    "        idx_assigned_anchors_boxes = [[0, None] for bbox in bboxes]\n",
    "        ground_truth_bboxes = []\n",
    "        # init anchor boxes\n",
    "        for s in range(len(self.S)):\n",
    "            S_size = self.S[s]\n",
    "            # divide the normalized image in S_size parts, then we take the size of each cell\n",
    "            cell_size = 1 / S_size\n",
    "            for idx in range(S_size*S_size):\n",
    "                x = idx%S_size\n",
    "                y = idx//S_size\n",
    "\n",
    "                xcenter = x*cell_size + cell_size/2\n",
    "                ycenter = y*cell_size + cell_size/2\n",
    "        \n",
    "                # add the 3 anchor bboxes to the cell\n",
    "                anchor_boxes = []\n",
    "                for j in range(3):\n",
    "                    # create anchor box: [xcenter, ycenter, width, height, object score, class, \n",
    "                    # coord_x, coord_y, cell_size, t_x, t_y, t_w, t_h]\n",
    "                    anchor_box = [xcenter, \n",
    "                                  ycenter, \n",
    "                                  self.anchors[s][j][0], \n",
    "                                  self.anchors[s][j][1], \n",
    "                                  0, \n",
    "                                  0,\n",
    "                                  x,\n",
    "                                  y,\n",
    "                                  cell_size,\n",
    "                                  0,\n",
    "                                  0, \n",
    "                                  0,\n",
    "                                  0\n",
    "                                 ]\n",
    "                    ground_truth_bboxes.append(anchor_box)\n",
    "                    \n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            temp_best_iou = 0\n",
    "            idx_best_anchor = None\n",
    "            idx_anchor_box = 0\n",
    "            for anchor in ground_truth_bboxes:\n",
    "                # check if center of image is inside the anchor's cell\n",
    "                cell_size = anchor[8]\n",
    "                # obtain box coordinates\n",
    "                box_cx = bbox[0] / cell_size\n",
    "                box_cy = bbox[1] / cell_size\n",
    "                # obtain anchor's cell coordinates\n",
    "                cx = anchor[6]\n",
    "                cy = anchor[7]\n",
    "                # continue only if box center is in anchor's cell\n",
    "                if box_cx >= cx and box_cx < cx+1 and box_cy >= cy and box_cy < cy+1:\n",
    "                    if box_cx == cx:\n",
    "                        box_cx += 1e-8\n",
    "                    if box_cy == cy:\n",
    "                        box_cy += 1e-8\n",
    "                    res_iou = IOU(bbox, anchor)\n",
    "                    # check if iou is better than previous and if the anchor box is free\n",
    "                    if res_iou > temp_best_iou and anchor[4] == 0:\n",
    "                        # free previous selected anchor if any\n",
    "                        if idx_best_anchor != None:\n",
    "                            if temp_best_iou >= self.thr_ignore:\n",
    "                                ground_truth_bboxes[idx_best_anchor][4] = -1\n",
    "                            else:\n",
    "                                # reset objectness and class\n",
    "                                ground_truth_bboxes[idx_best_anchor][4] = 0\n",
    "                                ground_truth_bboxes[idx_best_anchor][5] = 0\n",
    "                                # reset previous values for coordinates\n",
    "                                ground_truth_bboxes[idx_best_anchor][9:] = [0 for x in range(4)]\n",
    "                            \n",
    "                        # update values with new anchor\n",
    "                        temp_best_iou = res_iou\n",
    "                        idx_best_anchor = idx_anchor_box\n",
    "                        # set objectness and class\n",
    "                        anchor[4] = 1\n",
    "                        anchor[5] = bbox[5]\n",
    "                        # set t_x\n",
    "                        anchor[9] = get_tx(box_cx, cx)\n",
    "                        #print(f\"BOX_CY: {box_cy} COORDINATE Y: {cy}\")\n",
    "                        #print(f\"RES: {get_ty(box_cy, cy)}\")\n",
    "                        #print(f\"START TY, BOX_CY: {box_cy}, CY: {cy}\")\n",
    "                        anchor[10] = get_ty(box_cy, cy)\n",
    "                        #print(f\"RES: {anchor[10]}\")\n",
    "                        anchor[11] = get_tw(bbox[2], anchor[2])\n",
    "                        anchor[12] = get_th(bbox[3], anchor[3])\n",
    "                    \n",
    "                idx_anchor_box += 1\n",
    "  \n",
    "        img = Image.open(os.path.join(self.path_trainset, row[\"filename_img\"]))\n",
    "        return self.transform(img).float()/255, torch.Tensor(ground_truth_bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a85d071",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336a2df",
   "metadata": {},
   "source": [
    "## Load Darknet weights into yolo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a04604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_loss(pred, y):\n",
    "    lambda_obj = 1\n",
    "    lambda_box = 10\n",
    "    lambda_class = 1\n",
    "    lambda_noobj = 10\n",
    "    \n",
    "    # objectness loss\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    # takes coordinates of anchor bboxes imputed for objectnes loss\n",
    "    obj_loss_idx = y[..., 4] == 1\n",
    "    obj_loss = bce_loss(pred[obj_loss_idx][:,4], y[obj_loss_idx][:,4])\n",
    "    \n",
    "    # coordinates loss\n",
    "    mse_loss = nn.MSELoss()\n",
    "    box_loss = mse_loss(pred[obj_loss_idx][:, :4], y[obj_loss_idx][:, -4:])\n",
    "    \n",
    "    # class loss\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "    class_loss = cross_entropy_loss(pred[obj_loss_idx][:, 5:], y[obj_loss_idx][:, 5].type(torch.LongTensor).to(device))\n",
    "    \n",
    "    # no object loss\n",
    "    noobj_loss_idx = y[..., 4] == 0\n",
    "    noobj_loss = bce_loss(pred[noobj_loss_idx][:,4], y[noobj_loss_idx][:,4])\n",
    "    \n",
    "    return lambda_obj * obj_loss + lambda_box * box_loss + lambda_class * class_loss + lambda_noobj * noobj_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40953862",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = yolov3(yolo_architecture, 10).to(device)\n",
    "yolo_state_dict = yolo_model.state_dict()\n",
    "\n",
    "# load darknet weights\n",
    "darknet_state_dict = torch.load(\"./backup_models/darknet_weights\")\n",
    "for layer in darknet_state_dict:\n",
    "    # skip last layer of darknet\n",
    "    if layer.split(\".\")[1] != \"12\":\n",
    "        yolo_state_dict[layer] = darknet_state_dict[layer]\n",
    "yolo_model.load_state_dict(yolo_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a10185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (width, height)\n",
    "# from big grid to small\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "nuim_train = trainset('/home/loreleva/Big Data Project/new_dataset/train', ANCHORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "augmentation = A.Compose(\n",
    "    [\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "\n",
    "    ], bbox_params=A.BboxParams(format=\"yolo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ff6af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 21\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(f\"EPOCH: {i} BATCH: {n_batch}/{len(train_loader)} LOSS: {loss.item()}\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m         \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:410\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    408\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 28 EPOCHS, loss = 3.3306902824344022\n",
    "\n",
    "i=0\n",
    "batch_size = 8\n",
    "lr = 1e-3\n",
    "best_parameters = None\n",
    "best_loss = math.inf\n",
    "optimizer = optim.Adam(params=yolo_model.parameters(), lr=lr)\n",
    "while(True):\n",
    "    losses = []\n",
    "    n_batch = 0\n",
    "    train_loader = DataLoader(nuim_train, batch_size=batch_size, shuffle=True)\n",
    "    total_batches = len(train_loader)\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        detections = yolo_model(x)\n",
    "        detections = torch.cat([torch.reshape(detection,  (x.shape[0], -1, 15)) for detection in detections], 1)\n",
    "        loss = yolo_loss(detections, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(f\"EPOCH: {i} BATCH: {n_batch}/{len(train_loader)} LOSS: {loss.item()}\")\n",
    "        if n_batch % 1000 == 0:\n",
    "            print(f\"BATCH: {n_batch+1}/{total_batches}\")\n",
    "            print(f\"loss: {loss.item()}\\n\")\n",
    "        n_batch+=1\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    mean_losses = sum(losses)/len(losses)\n",
    "    if mean_losses < best_loss:\n",
    "        if best_loss != math.inf:\n",
    "            os.remove(f\"./backup_models/yolo_weights_loss_{best_loss}\")\n",
    "        best_loss = mean_losses\n",
    "        best_parameters = yolo_model.state_dict()\n",
    "        torch.save(best_parameters, f\"./backup_models/yolo_weights_loss_{best_loss}\")\n",
    "    print(f\"EPOCH: {i} MEAN LOSSES EPOCH: {sum(losses)/len(losses)}\")\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f8f2d",
   "metadata": {},
   "source": [
    "# Plot BBOXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10303cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUIMAGES_LABELS = [\n",
    "    \"human\",\n",
    "    \"barrier\",\n",
    "    \"trafficcone\",\n",
    "    \"bicycle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"motorcycle\",\n",
    "    \"truck\",\n",
    "    \"construction_vehicles\",\n",
    "    \"trailer\",\n",
    "]\n",
    "\n",
    "# color in format BGR\n",
    "label_to_color = {\n",
    "    \"human\" : (102, 0, 0),\n",
    "    \"barrier\" : (0, 0, 0),\n",
    "    \"trafficcone\" : (0, 97, 243),\n",
    "    \"bicycle\" : (0, 102, 0),\n",
    "    \"bus\" : (0, 176, 159),\n",
    "    \"car\" : (0, 0, 102),\n",
    "    \"motorcycle\" : (84, 105, 0),\n",
    "    \"truck\" : (99, 104, 81),\n",
    "    \"construction_vehicles\" : (0, 76, 153),\n",
    "    \"trailer\" : (76, 153, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bboxes(img, bboxes, type_bbox=\"yolo\", confidence=False, img_format=\"torch\"):\n",
    "    # when confidence=True, bbox is like: [coordinates, confidence, class]\n",
    "    fontScale = 1\n",
    "    thickness = 2\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    \n",
    "    # modify img array to make it compatible with cv2\n",
    "    if img_format == \"torch\":\n",
    "        img = torch.permute(img, (1, 2, 0)).numpy()\n",
    "    elif img_format == \"numpy\":\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "    if img_format != \"cv2\":\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    if img.dtype != np.uint8:\n",
    "        img *= 255\n",
    "        img = img.astype(np.uint8)\n",
    "        \n",
    "    img_h, img_w = img.shape[:2]\n",
    "    if type_bbox == \"yolo\":\n",
    "        # transforming yolo boxes in [xmin, ymin, xmax, ymax, class] type\n",
    "        bboxes = yolo_to_pascal(img_h, img_w, bboxes, confidence=confidence)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "            if confidence:\n",
    "                # add confidence score to text\n",
    "                label = NUIMAGES_LABELS[int(bbox[5])]\n",
    "                text =  label + f\" {bbox[4] * 100}%\"\n",
    "                color = label_to_color[label]\n",
    "            else:\n",
    "                text = NUIMAGES_LABELS[int(bbox[4])]\n",
    "                color = label_to_color[text]\n",
    "                \n",
    "            img = cv2.rectangle(img,\n",
    "                                (bbox[0], bbox[1]),\n",
    "                                (bbox[2], bbox[3]), \n",
    "                                color, \n",
    "                                2)\n",
    "            \n",
    "            text_size, _ = cv2.getTextSize(text, \n",
    "                                           font, \n",
    "                                           fontScale=fontScale, \n",
    "                                           thickness=thickness)\n",
    "            text_w, text_h = text_size\n",
    "            text_x, text_y = bbox[:2]\n",
    "            \n",
    "            # check if text goes out of the image\n",
    "            if text_x + text_w > img_w:\n",
    "                text_x = img_w - text_w\n",
    "            if text_y - text_h < 0:\n",
    "                text_y = 0\n",
    "            img = cv2.rectangle(img, \n",
    "                                (text_x, text_y), \n",
    "                                (text_x + text_w, text_y - text_h), \n",
    "                                color, \n",
    "                                -1)\n",
    "            img = cv2.putText(img, \n",
    "                              text, \n",
    "                              (text_x, text_y), \n",
    "                              font, \n",
    "                              fontScale=fontScale, \n",
    "                              color=(255, 255, 255),\n",
    "                              thickness=thickness)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
